\documentclass{scrartcl}

\usepackage{fixltx2e}

% Step environment
% <https://tex.stackexchange.com/a/12943/13262>
\usepackage{amsthm}
\newtheorem*{remark}{Remark}
%
\newtheoremstyle{named}{}{}{\itshape}{}{\bfseries}{.}{.5em}{\thmnote{#1 }#3}
\theoremstyle{named}
\newtheorem*{step}{Step}

\usepackage{microtype}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{tabularx}

\usepackage{pgfplots}
\pgfplotsset{compat=newest}

\usepackage{siunitx}

\newcommand\mytitle{How to evaluate and optimize color spaces against experimental data}
\newcommand\myauthor{Nico Schlömer}

\usepackage[
  pdfencoding=unicode,
  ]{hyperref}
\hypersetup{
  pdfauthor={\myauthor},
  pdftitle={\mytitle}
}

% <https://tex.stackexchange.com/a/43009/13262>
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\usepackage[T1]{fontenc}
\usepackage{newtxtext}
\usepackage{newtxmath}

% degree symbol
\usepackage{gensymb}

% % <https://tex.stackexchange.com/a/413899/13262>
% \usepackage{etoolbox}
% \makeatletter
% \long\def\etb@listitem#1#2{%
%   \expandafter\ifblank\expandafter{\@gobble#2}
%     {}
%     {\expandafter\etb@listitem@i
%      \expandafter{\@secondoftwo#2}{#1}}}
% \long\def\etb@listitem@i#1#2{#2{#1}}
% \makeatother

% Okay. Don't use biblatex/biber for now. There are breaking changes in every
% revision, and we'd have to stick to the exact version that arxiv.org has,
% otherwise it's error messages like
% ```
% Package biblatex Warning: File 'main.bbl' is wrong format version
% - expected 2.8.
% ```
% \usepackage[sorting=none]{biblatex}
% \bibliography{bib}

\usepackage{amsmath}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator*{\argmin}{arg\,min}

% \usepackage{amsfonts}
\usepackage{bm}
\newcommand\R{\ensuremath{\mathbb{R}}}
\newcommand\xt{\ensuremath{\bm{\tilde{x}}}}
\newcommand\yt{\ensuremath{\bm{\tilde{y}}}}

\title{\mytitle\footnote{The LaTeX sources of this article are on
\url{https://github.com/nschloe/colorio}}}
\author{\myauthor}

\begin{document}

\maketitle
\begin{abstract}
  todo
\end{abstract}

\section{Introduction}

Every year, articles on new color spaces are created, claiming that they comply to
certain experimental data better than other color spaces. In many cases, numbers are
provided in form of a table showing that the new color space is indeed better.
Unfortunately, it is hardly ever explained how those numbers are computed.

This article describes in detail how to assess the experimental compliance of color
space with experimental data. The article focuses on the two most used types of
experimental data: Hue linearity \cite{ebner,xiao,hung} and color difference ellipses
\cite{macadam1942,luorigg}.

Almost every color space is defined by a transformation $T$ that maps the CIE-1931-XYZ
coordinates into a three-dimensional new coordinate space (e.g., LAB). The
transformation is usually continuously differentiable and bijective. When talking about
a colorspace, one almost always means the transformation $T$ and its inverse.


TODO desirable properties of cost functionals:
\begin{itemize}
  \item Are continuous, and ideally continuously differentiable (for optimization)
  \item Are 0 if the experimental data is matched exactly (will never be fulfilled since
    experimental data contains errors). Together with the first one, number are small if
    the match is approximate.
  \item are scaling invariant
  \item are rotation invariant
  \item are translation invariant
\end{itemize}

% There are numerous experiments~\cite{ebner,xiao,hung,macadam,macadam,luo} which try to
% gauge perceptual distances between colors or to determine which colors of different
% luminosity are perceived as the same chroma.
%
% These experimental data have been used in the past to approximate a perceptually uniform
% colorspace, i.e., a color space in which the Euclidean distance represents the perceived
% distance, and in which colors of same chroma all sit in one line. (These two goals are
% mutually inclusive.) So far, the general approach was to assume that the transformation
% from XYZ space takes a certain mathematical form with a number of free parameters, e.g.,
% the parameters $e$, $\alpha_{i,j}$, and $\omega_{i,j}$.
% \begin{equation}\label{eq:safdar}
%   \begin{split}
%     \begin{bmatrix}
%       L\\M\\S
%     \end{bmatrix}
%     =
%     \begin{bmatrix}
%       \alpha_{1,1} & \alpha_{1,2} & 1 - \alpha_{1,1} - \alpha_{1,2}\\
%       \alpha_{2,1} & \alpha_{2,2} & 1 - \alpha_{2,1} - \alpha_{2,2}\\
%       \alpha_{3,1} & \alpha_{3,2} & 1 - \alpha_{3,1} - \alpha_{3,2}
%     \end{bmatrix}
%     \begin{bmatrix}
%       X_{D65}\\Y_{D65}\\Z_{D65}
%     \end{bmatrix}\\
%     \{L',M',S'\} = \left(\frac{c_1 + c_2\left(\frac{\{L,M,S\}}{10000}\right)^n}{1 + c_3\left(\frac{\{L,M,S\}}{10000}\right)^n}\right)^{pe}\\
%     \begin{bmatrix}
%       I_z\\a_z\\b_z
%     \end{bmatrix}
%     =
%     \begin{bmatrix}
%       \omega_{1,1} & \omega_{1,2} & 1 - \omega_{1,1} - \omega_{1,2}\\
%       \omega_{2,1} & \omega_{2,2} &   - \omega_{2,1} - \omega_{2,2}\\
%       \omega_{3,1} & \omega_{3,2} &   - \omega_{3,1} - \omega_{3,2}
%     \end{bmatrix}
%     \begin{bmatrix}
%       L'\\M'\\S'
%     \end{bmatrix}\\
%   \end{split}
% \end{equation}
% in~\cite{safdar}. Then, an optimization algorithm was applied to retrieve those
% parameters which best match the given experimental data. The resulting color space has
% then been declared ``optimal'', which held true until a new article with a different
% assumption was published which, almost by chance, achieved even better accordance with
% the data.
%
% The assumption that the transformation of XYZ to the perceptually uniform color space
% takes a particular form is of course of practical nature: A low-dimensional parameter
% space is easy to search. However, to put it mildly, it is quite optimistic to assume
% that the visual system of the brain (see figure~\ref{fig:monkey}) does a transformation that
% can be expressed in terms of some linear transformations and elementary mathematical
% functions.
%
% TODO why doesn't polynomial approximation, pade not work? runge!
%
% \begin{figure}
%   \centering
%   \includegraphics[width=0.3\textwidth]{images/monkey.png}
%   \caption{Wiring diagram of the visual system of the macaque monkey, reproduced
%   from~\cite{felleman}.}
%   \label{fig:monkey}
% \end{figure}
%
% This article describes a much more general approach and succeeds in finding a color
% space that is far more perceptually uniform that everything that has been found so far.
% Even more, there can be no color space matching the given experimental data even better.
%
% \section{Optimization problem}
%
% There are many ideas that generalize the few-parameter approaches like~\ref{eq:safdar}.
% What comes to mind are polynomial approximations
% \begin{equation}\label{eq:poly}
%   p_{\alpha}(x, y) = \sum_{i+j\le n} \alpha_{i,j} x^i y^j
% \end{equation}
% where optimization happens over the coefficients $\alpha_{i,j}$, or even fractional
% polynomials,
% \[
%   r_{\alpha, \beta}(x, y) = \frac{p_\alpha(x,y)}{q_\beta(x, y)}
% \]
% where both numerator and denominator are of the form~\ref{eq:poly} (Padé approximant).
% Both of the approaches offer the advantage that -- given an infinite source of
% experimental data -- the actual transformation $t$ can be approximated arbitrarily well
% with increased polynomial degrees. Unfortunately, polynomial approximations suffer from
% Runge's phenomenon, meaning that naively chosen reference points for the optimization
% can lead solutions which approximate $t$ well at those points, but very badly everywhere
% else. See section~\ref{sec:polyfail}.
%
% This article takes a more robust approach. The key idea is to divide the domain into
% many small triangles (see figure~\ref{fig:triangles}) and to allow each of the nodes to
% move around more or less freely such that the resulting shape matches the experimental
% data well. All continuous transformations can be approximated by this approach so it is
% reasonable to assume that we do not restrict ourselves too much here.
%
% The mathematical concept
% \[
%   F(x, y) = \begin{bmatrix}a(x,y)\\b(x,y)\end{bmatrix}
% \]
% where both $a:\Omega\to\R^2$ and $b:\Omega\to\R^2$ are piecewise linear functions on the
% triangles.
%
% Each of the two
% \[
% fl
% \]
% \[
%   \begin{split}
%   F(a_x, a_y) &=\\
%   &F_{\text{Hung--Berns}}(a_x, a_y) +
%   F_{\text{Ebner--Fairchild}}(a_x, a_y) +
%   F_{\text{Xiao}}(a_x, a_y) +\\
%   &F_{\text{MacAdam}}(a_x, a_y) +
%   F_{\text{Luo--Rigg}}(a_x, a_y) +\\
%   &F_{\Delta}(a_x, a_y)
%   \end{split}
% \]
%
% % This is achieved by dividing the xy-triangle into many smaller triangles, i.e., nodes
% % and cells, and allowing each of the nodes to move around freely to match the
% % experimental data. This is the most general approach possible; all smooth
% % transformations can be expressed in this way.
%
% TODO all cost functions must be invariant to
% \begin{itemize}
%   \item rotation,
%   \item scaling, and
%   \item translation.
% \end{itemize}

Let $(X_i, Y_i, Z_i)\in\R^3$ be a given set of points in XYZ space which, according to
some experiment, are of equal perceived hue. Consider the two non-lightness coordinates
of their image $(x_i, y_i) \coloneqq T(X_i, Y_i, Z_i)$ (see figure TODO). What is
measure of how well the points $(x_i, y_i)$ sit on a straight line? The general idea
here is to cast a line through ``the middle'' of the point cloud and sum up the
distances of all points to that line. There are multiple meaningful ways in which
``the middle'' and ``distance'' can be defined. Remember that in $\R^n$, the distance
between two points $a$ and $b$ can be defined by a \emph{norm}.


\subsection{Cost functional for target distances and ellipses}

Some data sets give target distance values $d_i$ for pairs of colors $X_{i,1}, X_{i,2}$
(e.g. \cite{macadam1974}). A color space conforms with this data if the distance
between the transformed points
$\delta_i \coloneqq \norm{T(X_{i,1}) - T(X_{i,2})}_2$
is close to a scaled $d_i$, i.e.,
\begin{equation}\label{eq:p}
  p_2
  \coloneqq \sqrt{\sum_{i=1}^n (\alpha d_i - \delta_i)^2}
\end{equation}
The parameter $\alpha$ is chosen to minimize the expression, namely, $\alpha =
\left.\sum_{i=1}^n d_i \delta_i \middle/ \sum_{i=1}^n d_i^2\right.$.
To achieve scale-invariance, one often divides by
$\sqrt{\sum_{i=1}^n \delta_i^2}$.
The resulting expression is always between 0 and 1, and after scaling by 100, this is
called the STRESS metric for target distances,
\begin{equation}\label{eq:p}
  p_{\text{STRESS}}
  \coloneqq
  100
  \frac{%
    \sqrt{\sum_{i=1}^n (\alpha d_i - \delta_i)^2}
  }{
    \sqrt{\sum_{i=1}^n \delta_i^2}
  }.
\end{equation}

\begin{figure}
  \centering
  \input{pstress.tex}
  \caption{$p_\text{STRESS}$ for a number of color spaces. The color space OSA-UCS was
  designed specifically with the MacAdam \cite{macadam1974} dataset in mind. Both CAM
  spaces perform well for all data sets.}
\end{figure}

A common special case of this setting is distance data given in terms of ellipse points,
i.e., a color center with a number of standard deviations (or similar) in various
directions. The famous MacAdam ellipses (\cite{macadam1942}) are derived from such data.
Since the ellipses are supposed to to be circles of equal size in the transformed space,
the target value for all $d_i$ is 1. The parameter $\alpha$ simplifies to be the average
over all $\delta_i$ and we have
\begin{equation}\label{eq:e}
  e_{\text{STRESS}}
  \coloneqq
  100
  \frac{%
    \sqrt{\sum_{i=1}^n (\alpha - \delta_{i})^2}
  }{
    \sqrt{\sum_{i=1}^n \delta_i^2}
  }.
\end{equation}
In case only the actual ellipses are given (e.g., \cite{luorigg}), one can place a
number of points onto their boundaries (e.g., 8 or 16) and compute the residual with
from them.

\begin{table}
  \centering
  \begin{tabular}{lrr}
    \toprule
    & MacAdam & Luo-Rigg(8) \cite{luorigg}\\
    \midrule
CAM02 (UCS)   & \textbf{27.2} &         30.1\\
CAM16 (UCS)   &         29.0  & \textbf{28.4}\\
CIELAB        &         57.3  &         48.5\\
CIELUV        &         36.5  &         45.3\\
IPT           &         38.5  &         47.5\\
$IC_TC_P$     &         75.9  &         61.2\\
$J_z a_z b_z$ &         40.7  &         40.3\\
Oklab         &         37.7  &         39.1\\
OSA-UCS       &         63.6  &         42.2\\
RLAB          &         46.6  &         46.6\\
xyY           &         53.8  &         44.0\\
    \bottomrule
  \end{tabular}
  \caption{$e_\text{STRESS}$ for a number of color spaces. CAM02 and its close relative
  CAM16 far outperform the other color spaces, particularly for the Luo-Rigg data set.}
\end{table}

\subsection{Cost functional for hue linearity}

There are multiple experiments which data about which colors are perceived to be of
equal hue~\cite{hung,ebner,xiao} (see figure~\ref{}). Color spaces are considered good
if the transformation maps points of equal perceived hue onto a straight line.

What is a good measure of how well points sit on a straight line?
A common idea is to find the straight line that mimizes the sum of squared distances to
all points. This general approach is usually referred to as \emph{total least squares
(TLS)}.
The line is typically
given implicitly by $\alpha_1 x + \alpha_2 y
= 0$ with $\alpha_1,\alpha_2\in\R$, $\|\alpha\|_2^2 = \alpha_1^2 + \alpha_2^2 = 1$.
Because this assumes that the line passes through the origin, all points are
first translated such that the whitepoint sits in in origin.

For scale-invariance, it is convenient to divide by the maximizer of the squared
distances, in total:
\begin{equation}\label{eq:l}
l_2 \coloneqq
  \frac{
\min_{\|\alpha\|_2=1}
  \sqrt{\sum_{i=1}^n (\alpha_1 \tilde{x}_i + \alpha_2 \tilde{y}_i)^2}
}{
\max_{\|\alpha\|_2=1}
  \sqrt{\sum_{i=1}^n (\alpha_1 \tilde{x}_i + \alpha_2 \tilde{y}_i)^2}
}
\end{equation}
with the translated sample points
\[
  \tilde{x}_i \coloneqq x_i-w_x,\qquad
  \tilde{y}_i \coloneqq y_i-w_y.
\]

The value of $l_2$ can be approximated with any appropriate optimization method. A more
explicit and more easily computable representation however is retrieved as follows.
With the $n$-by-2 coordinate matrix
\[
  A \coloneqq \begin{pmatrix}
    \tilde{x}_1 & \tilde{y}_1\\
    \vdots & \vdots\\
    \tilde{x}_n & \tilde{y}_n
  \end{pmatrix},
\]
the sum in \eqref{eq:l} can be written as
\[
  \sum_{i=1}^n (\alpha_1 \tilde{x}_i + \alpha_2 \tilde{y}_i)^2
  = (A \alpha)^T (A \alpha)
  = \alpha^T A^T A \alpha.
\]
This makes clear that $l_2$ is exactly the ratio of the square roots of the two
eigenvalues of $A^TA$ or equivalently the ratio of the two singular values of $A$,
\[
l_2
= \frac{
  \sqrt{\lambda_{\min}(A^T A)}
  }{
    \sqrt{\lambda_{\max}(A^T A)}
  }
= \frac{\sigma_{\min}(A)}{\sigma_{\max}(A)}.
\]
The value is given explicitly by
\begin{equation*}
  l_2 = \sqrt{
    \frac{
      \xt^T\xt
      + \yt^T\yt
      - \sqrt{(\xt^T\xt - \yt^T\yt)^2 + 4 (\xt^T\yt)^2}
    }{
      \xt^T\xt
      + \yt^T\yt
      + \sqrt{(\xt^T\xt - \yt^T\yt)^2 + 4 (\xt^T\yt)^2}
    }
    }.
\end{equation*}
The expression under the outer root is indeed always nonnegative by virtue of the
Cauchy-Schwarz inequality $(\xt^T\yt)^2 \le (\xt^T\xt) (\yt^T\yt)$. It is 0 if and only
if $\xt$ and $\yt$ are linearly dependent, i.e., if the points $(x_i, y_i)$ sit on a
straight line through the origin.

Since $0\le l_2\le 1$, the value can be given in terms of STRESS,
\begin{equation}\label{eq:lstress}
  l_\text{STRESS} = 100 \frac{\sigma_{\min}(A)}{\sigma_{\max}(A)}.
\end{equation}


% \begin{remark}
%   The representation \eqref{eq:s2} is suitable for optimization purposes. Since a value
%   $\sqrt{t}$ is small if and only if $t$ is small, one would in the interest of
%   simplicity disregard the outer square root.
% \end{remark}

\begin{remark}
Besides \eqref{eq:l}, there are other meaningful ways in which the distance of a point
cloud to a line can be defined. Instead of summing the squared distances, one could take
the $p$-norm,
\[
l_p
  \coloneqq
  \frac{
    \min_{\|\alpha\|_2=1} \|\alpha_1 \tilde{x} + \alpha_2\tilde{y}\|_p
  }{
    \max_{\|\alpha\|_2=1} \|\alpha_1 \tilde{x} + \alpha_2\tilde{y}\|_p
  }
  = \frac{
    \min_{\|\alpha\|_2=1}
  \left(\sum_{i=1}^n |\alpha_1 \tilde{x}_i + \alpha_2 \tilde{y}_i|^p\right)^{1/p}
}{
    \max_{\|\alpha\|_2=1}
  \left(\sum_{i=1}^n |\alpha_1 \tilde{x}_i + \alpha_2 \tilde{y}_i|^p\right)^{1/p}
}
\]
  with $1\le p \le \infty$.
  The value $l_2$ remains of prominent importance, though, because of its smooth
  dependence on the point set.
\end{remark}


\begin{figure}
  \centering
  \input{lstress.tex}
  \caption{$l_\text{STRESS}$ averaged over all arms of each data set.  While all listed
  color spaces perform quite well, OSA-UCS is the overall best color space for hue
  linearity.}
\end{figure}


% \printbibliography{}
\bibliography{main}{}
\bibliographystyle{plain}

\end{document}
