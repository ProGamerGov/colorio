\documentclass{scrartcl}

\usepackage{fixltx2e}

% Step environment
% <https://tex.stackexchange.com/a/12943/13262>
\usepackage{amsthm}
\newtheorem*{remark}{Remark}
%
\newtheoremstyle{named}{}{}{\itshape}{}{\bfseries}{.}{.5em}{\thmnote{#1 }#3}
\theoremstyle{named}
\newtheorem*{step}{Step}

\usepackage{microtype}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{tabularx}

\usepackage{pgfplots}
\pgfplotsset{compat=newest}

\usepackage{siunitx}

\newcommand\mytitle{How to evaluate and optimize color spaces against experimental data}
\newcommand\myauthor{Nico Schlömer}

\usepackage[
  pdfencoding=unicode,
  ]{hyperref}
\hypersetup{
  pdfauthor={\myauthor},
  pdftitle={\mytitle}
}

% <https://tex.stackexchange.com/a/43009/13262>
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclareMathOperator{\avg}{avg}

\usepackage[T1]{fontenc}
\usepackage{newtxtext}
\usepackage{newtxmath}

% degree symbol
\usepackage{gensymb}

% % <https://tex.stackexchange.com/a/413899/13262>
% \usepackage{etoolbox}
% \makeatletter
% \long\def\etb@listitem#1#2{%
%   \expandafter\ifblank\expandafter{\@gobble#2}
%     {}
%     {\expandafter\etb@listitem@i
%      \expandafter{\@secondoftwo#2}{#1}}}
% \long\def\etb@listitem@i#1#2{#2{#1}}
% \makeatother

% Okay. Don't use biblatex/biber for now. There are breaking changes in every
% revision, and we'd have to stick to the exact version that arxiv.org has,
% otherwise it's error messages like
% ```
% Package biblatex Warning: File 'main.bbl' is wrong format version
% - expected 2.8.
% ```
% \usepackage[sorting=none]{biblatex}
% \bibliography{bib}

\usepackage{amsmath}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator*{\argmin}{arg\,min}

% \usepackage{amsfonts}
\usepackage{bm}
\newcommand\rr{\ensuremath{\bm{r}}}
\newcommand\R{\ensuremath{\mathbb{R}}}
\newcommand\xt{\ensuremath{\bm{\tilde{x}}}}
\newcommand\yt{\ensuremath{\bm{\tilde{y}}}}

\title{\mytitle\footnote{The LaTeX sources of this article are on
\url{https://github.com/nschloe/colorio}}}
\author{\myauthor}

\begin{document}

\maketitle
\begin{abstract}
  todo
\end{abstract}

\section{Introduction}

Every year, articles on new color spaces are created, claiming that they comply to
certain experimental data better than other color spaces. In many cases, numbers are
provided in form of a table showing that the new color space is indeed better.
Unfortunately, it is hardly ever explained how those numbers are computed.

This article describes in detail how to assess the experimental compliance of color
space with experimental data. The article focuses on the two most used types of
experimental data: Hue linearity \cite{ebner,xiao,hung} and color difference ellipses
\cite{macadam1942,luorigg}.

Almost every color space is defined by a transformation $T$ that maps the CIE-1931-XYZ
coordinates into a three-dimensional new coordinate space (e.g., LAB). The
transformation is usually continuously differentiable and bijective. When talking about
a colorspace, one almost always means the transformation $T$ and its inverse.


TODO desirable properties of cost functionals:
\begin{itemize}
  \item Are continuous, and ideally continuously differentiable (for optimization)
  \item Are 0 if the experimental data is matched exactly (will never be fulfilled since
    experimental data contains errors). Together with the first one, number are small if
    the match is approximate.
  \item are scaling invariant
  \item are rotation invariant
  \item are translation invariant
\end{itemize}


\section{Average, root mean square, and how to combine results}

In several stages of the assessment or optimization process, we will have to combine
several positive result values (e.g., residuals from different data sets) into one.
There are multiple meaningful ways of doing so. The most common are certainly the
\emph{average} and the \emph{root mean square} of different results
$\rr = \{r_i\}_{i=1}^n$,
\[
  \avg_1(\rr) = \frac{1}{n}\sum_{i=1}^n |r_i|,\quad
  \avg_2(\rr) = \sqrt{\frac{1}{n}\sum_{i=1}^n |r_i|^2}.
\]
This is readily generalized to the $p$-average
\[
  \avg_p(\rr) \coloneqq \left(\frac{1}{n}\sum_{i=1}^n |r_i|^p\right)^{1/p} =
  \frac{1}{n^{1/p}} \|\rr\|_p
\]
with any given $0 < p < \infty$. In the limits $p\to 0+$ and $p\to +\infty$, this
becomes the \emph{geometric mean} and the \emph{maximum}
\[
  \avg_0(\rr) \coloneqq \left(\prod_{i=1}^n |r_i|\right)^{1/n}, \quad
  \avg_{\infty}(\rr) \coloneqq \max_{i\in\{1,\dots,n\}} |r_i|.
\]
Note that, for every $\rr$, $\avg_p(\rr)$ as a function in $p$ is smooth and monotonic
increasing
\[
  \avg_0(\rr) \le \avg_p(\rr) \le \avg_q(\rr) \le \avg_{\infty}(\rr) \quad (p\le q).
\]
(Compare figure \ref{fig:1}.)
Hence, the larger $p$, the closer $\avg_p(\rr)$ will be to $\max(|\rr|)$.
This makes a larger $p$ suitable, for example, for an optimization process that tries to
minimize the maximum STRESS (see section~\ref{}).

Choosing $p < 1$, on the other hand, puts more emphasis on the smaller values in $\rr$.
In particular, $\avg_0(\rr)=0$ if only one $r_i=0$. One almost never wants this in the
context of experimental validation or optimization.

\begin{figure}
  \centering
  \input{figures/averages.tex}
  \caption{$p$-averages for the three sequences of length $n=20$, $s_\text{equal} = \{1,
  \dots, 1\}$, $s_\text{uniform} = \{0, h, \dots,
  1\}$ (with $h = 1 / (n-1)$), and $s_\text{outlier} = \{1, 0,\dots, 0\}$. Note how the
  $\avg_p$ are monotonic increasing and smooth. For large $p$ (not shown), all curves
  approach the maximum value in the set, $1$.}
  \label{fig:1}
\end{figure}


\section{Residuals and STRESS}

% There are numerous experiments~\cite{ebner,xiao,hung,macadam,macadam,luo} which try to
% gauge perceptual distances between colors or to determine which colors of different
% luminosity are perceived as the same chroma.
%
% These experimental data have been used in the past to approximate a perceptually uniform
% colorspace, i.e., a color space in which the Euclidean distance represents the perceived
% distance, and in which colors of same chroma all sit in one line. (These two goals are
% mutually inclusive.) So far, the general approach was to assume that the transformation
% from XYZ space takes a certain mathematical form with a number of free parameters, e.g.,
% the parameters $e$, $\alpha_{i,j}$, and $\omega_{i,j}$.
% \begin{equation}\label{eq:safdar}
%   \begin{split}
%     \begin{bmatrix}
%       L\\M\\S
%     \end{bmatrix}
%     =
%     \begin{bmatrix}
%       \alpha_{1,1} & \alpha_{1,2} & 1 - \alpha_{1,1} - \alpha_{1,2}\\
%       \alpha_{2,1} & \alpha_{2,2} & 1 - \alpha_{2,1} - \alpha_{2,2}\\
%       \alpha_{3,1} & \alpha_{3,2} & 1 - \alpha_{3,1} - \alpha_{3,2}
%     \end{bmatrix}
%     \begin{bmatrix}
%       X_{D65}\\Y_{D65}\\Z_{D65}
%     \end{bmatrix}\\
%     \{L',M',S'\} = \left(\frac{c_1 + c_2\left(\frac{\{L,M,S\}}{10000}\right)^n}{1 + c_3\left(\frac{\{L,M,S\}}{10000}\right)^n}\right)^{pe}\\
%     \begin{bmatrix}
%       I_z\\a_z\\b_z
%     \end{bmatrix}
%     =
%     \begin{bmatrix}
%       \omega_{1,1} & \omega_{1,2} & 1 - \omega_{1,1} - \omega_{1,2}\\
%       \omega_{2,1} & \omega_{2,2} &   - \omega_{2,1} - \omega_{2,2}\\
%       \omega_{3,1} & \omega_{3,2} &   - \omega_{3,1} - \omega_{3,2}
%     \end{bmatrix}
%     \begin{bmatrix}
%       L'\\M'\\S'
%     \end{bmatrix}\\
%   \end{split}
% \end{equation}
% in~\cite{safdar}. Then, an optimization algorithm was applied to retrieve those
% parameters which best match the given experimental data. The resulting color space has
% then been declared ``optimal'', which held true until a new article with a different
% assumption was published which, almost by chance, achieved even better accordance with
% the data.
%
% The assumption that the transformation of XYZ to the perceptually uniform color space
% takes a particular form is of course of practical nature: A low-dimensional parameter
% space is easy to search. However, to put it mildly, it is quite optimistic to assume
% that the visual system of the brain (see figure~\ref{fig:monkey}) does a transformation that
% can be expressed in terms of some linear transformations and elementary mathematical
% functions.
%
% TODO why doesn't polynomial approximation, pade not work? runge!
%
% \begin{figure}
%   \centering
%   \includegraphics[width=0.3\textwidth]{images/monkey.png}
%   \caption{Wiring diagram of the visual system of the macaque monkey, reproduced
%   from~\cite{felleman}.}
%   \label{fig:monkey}
% \end{figure}
%
% This article describes a much more general approach and succeeds in finding a color
% space that is far more perceptually uniform that everything that has been found so far.
% Even more, there can be no color space matching the given experimental data even better.
%
% \section{Optimization problem}
%
% There are many ideas that generalize the few-parameter approaches like~\ref{eq:safdar}.
% What comes to mind are polynomial approximations
% \begin{equation}\label{eq:poly}
%   p_{\alpha}(x, y) = \sum_{i+j\le n} \alpha_{i,j} x^i y^j
% \end{equation}
% where optimization happens over the coefficients $\alpha_{i,j}$, or even fractional
% polynomials,
% \[
%   r_{\alpha, \beta}(x, y) = \frac{p_\alpha(x,y)}{q_\beta(x, y)}
% \]
% where both numerator and denominator are of the form~\ref{eq:poly} (Padé approximant).
% Both of the approaches offer the advantage that -- given an infinite source of
% experimental data -- the actual transformation $t$ can be approximated arbitrarily well
% with increased polynomial degrees. Unfortunately, polynomial approximations suffer from
% Runge's phenomenon, meaning that naively chosen reference points for the optimization
% can lead solutions which approximate $t$ well at those points, but very badly everywhere
% else. See section~\ref{sec:polyfail}.
%
% This article takes a more robust approach. The key idea is to divide the domain into
% many small triangles (see figure~\ref{fig:triangles}) and to allow each of the nodes to
% move around more or less freely such that the resulting shape matches the experimental
% data well. All continuous transformations can be approximated by this approach so it is
% reasonable to assume that we do not restrict ourselves too much here.
%
% The mathematical concept
% \[
%   F(x, y) = \begin{bmatrix}a(x,y)\\b(x,y)\end{bmatrix}
% \]
% where both $a:\Omega\to\R^2$ and $b:\Omega\to\R^2$ are piecewise linear functions on the
% triangles.
%
% Each of the two
% \[
% fl
% \]
% \[
%   \begin{split}
%   F(a_x, a_y) &=\\
%   &F_{\text{Hung--Berns}}(a_x, a_y) +
%   F_{\text{Ebner--Fairchild}}(a_x, a_y) +
%   F_{\text{Xiao}}(a_x, a_y) +\\
%   &F_{\text{MacAdam}}(a_x, a_y) +
%   F_{\text{Luo--Rigg}}(a_x, a_y) +\\
%   &F_{\Delta}(a_x, a_y)
%   \end{split}
% \]
%
% % This is achieved by dividing the xy-triangle into many smaller triangles, i.e., nodes
% % and cells, and allowing each of the nodes to move around freely to match the
% % experimental data. This is the most general approach possible; all smooth
% % transformations can be expressed in this way.


Let $(X_i, Y_i, Z_i)\in\R^3$ be a given set of points in XYZ space which, according to
some experiment, are of equal perceived hue. Consider the two non-lightness coordinates
of their image $(x_i, y_i) \coloneqq T(X_i, Y_i, Z_i)$ (see figure TODO). What is
measure of how well the points $(x_i, y_i)$ sit on a straight line? The general idea
here is to cast a line through ``the middle'' of the point cloud and sum up the
distances of all points to that line. There are multiple meaningful ways in which
``the middle'' and ``distance'' can be defined. Remember that in $\R^n$, the distance
between two points $a$ and $b$ can be defined by a \emph{norm}.


\subsection{Cost functional for target distances and ellipses}

Some data sets give target distance values $d_i$ for pairs of colors $X_{i,1}, X_{i,2}$.
Notable examples are the following.
\begin{itemize}
  \item MacAdam (1942) \cite{macadam1942}.
     25 ellipses,
     1 observer,
  \item MacAdam (1974) \cite{macadam1974}.
     128 pairs of colored tiles,
     49 to 76 observers,
     The CIE specifications for D65 and for the 1964
     supplementary observer for 100 visual field
     at least 500 lm/m2
   \item BFD-P (1986) \cite{luorigg}.

   \item Witt (1998) \cite{witt}.
     Illumination by a high-quality D65 light source
     Samples field size 10 degrees x 10 degrees of visual angle
     Illuminance 1300 lx
     10 to 14 observers
     418 sample pairs
     The data in the original article contains printing errors

  \item Leeds ()

  \item RIT-DuPont

  \item COMBVD.
    This collection is the union of BFD-P, Leeds, RIT-DuPont, and Witt, weighted such
    that all subsets contribute about equally, i.e., BFD-P with factor 1, Leeds 9,
    RIT-DuPont 9, Witt 7.
\end{itemize}

A color space conforms with this data if the distance between the transformed points
$\delta_i \coloneqq \norm{T(X_{i,1}) - T(X_{i,2})}_2$
is close to the scaled distance $\alpha d_i$, i.e.,
\begin{equation}\label{eq:p}
  p_2
  \coloneqq \sqrt{\sum_{i=1}^n (\alpha d_i - \delta_i)^2}
\end{equation}
where the parameter $\alpha$ is chosen to minimize the expression, namely, $\alpha =
\left.\sum_{i=1}^n d_i \delta_i \middle/ \sum_{i=1}^n d_i^2\right.$. This scaling makes
sure that the expression in invariant for scaling of $d$, too. To achieve
scale-invariance in $\delta$, one divides by $\sqrt{\sum_{i=1}^n \delta_i^2}$.  The
resulting expression is always between 0 and 1, and after scaling by 100, this is called
the STRESS (STandardized REsidual Sum of Squares) metric for target distances,
\begin{equation}\label{eq:p}
  p_{\text{STRESS}}
  \coloneqq
  100
  \frac{%
    \sqrt{\sum_{i=1}^n (\alpha d_i - \delta_i)^2}
  }{
    \sqrt{\sum_{i=1}^n \delta_i^2}
  }.
\end{equation}

\begin{remark}
One could as well have put the minimizing parameter at $\delta_i$ and divided by $\sum
  d_i^2$; the resulting expression is equal to $p_2$.
\end{remark}

\begin{remark}
Instead of the $2$-norm, one could in principle have chosen any other $0\le p\le\infty$
  as well,
\[
p_p = \left(\sum_{i=1}^n |\alpha d_i - \delta_i|^p\right)^{1/p}.
\]
The reason why $p_2$ is always chosen in practice is that the global minimizer $\alpha$
  is easily determined and the dependence on the data set is smooth.
\end{remark}

\begin{remark}
  There are other reasonable variants of $p_\text{STRESS}$, e.g.,
  \[
    \tilde{p}_\text{STRESS} = 100 \frac{\sqrt{\sum_{i=1}^n \frac{(\tilde{\alpha} d_i -
    \delta_i)^2}{\delta_i}}}{\sqrt{\sum_{i=1}^n \delta_i}}.
  \]
  As opposed to $p_\text{STRESS}$, $\tilde{p}_\text{STRESS}$ is  large if any $\delta_i$
  is small or vanishes.
  It measures the \emph{relative} deviation, i.e., the value of $\delta_i=\alpha d_i / r
  $ is the same as for $\delta_i = r \alpha d_i$ for any $r > 0$.  This can be useful in
  optimization when absolutely trying to prevent a collapsing of color distances or
  ellipses.

  Another such variant is
  \[
    \hat{p}_\text{STRESS} = 100 \sqrt{\sum_{i=1}^n \frac{(\hat{\alpha} d_i -
    \delta_i)^2}{\delta_i^2}}.
  \]
  It is less useful as the value of each of the summands is limited by $1$ for growing
  $\delta_i$.

  See figure~\ref{fig:norm-scaling}.
\end{remark}

\begin{figure}
  \label{fig:norm-scaling}
  \centering
  \input{figures/norm-scaling.tex}
  \caption{Different possibilities for the norm-scaling in $p_\text{STRESS}$ with target
  distance 1. The common variant $(1-x)^2$ is a simple quadratic.}
\end{figure}

\begin{figure}
  \centering
  \input{figures/macadam1974-xyy.tex}
  \hfill
  \input{figures/macadam1974-cielab.tex}
  \hfill
  \input{figures/macadam1974-cam16.tex}
  \caption{MacAdam 1974 color distance data in three color spaces. If the arrows meet,
  the dots are as far apart as prediced by MacAdam. Note that only those 43 color pairs
  are shown which are of approximately equal lightness.}
\end{figure}

\begin{figure}
\centering
\input{figures/pstress.tex}
\caption{$p_\text{STRESS}$ for a number of color spaces. The color space OSA-UCS was
designed specifically with the MacAdam \cite{macadam1974} dataset in mind. Both CAM
spaces perform well for all data sets. The MacAdam ellipses are assumed to exist without
  signficant change in a wide luminosity range, and is presented with $Y=50$ here.}
\end{figure}

\begin{figure}
\centering
  \input{figures/pstress-diff.tex}
  \caption{$p_\text{STRESS}$ for a number of color diffence formulae.
  Note that CIE76 is really just the Euclidean distance in CIELAB.}
\end{figure}

A common special case of this setting is distance data given in terms of ellipse points,
i.e., a color center with a number of standard deviations (or similar) in various
directions. The famous MacAdam ellipses \cite{macadam1942} are derived from such data.
Since the ellipses are supposed to to be circles of equal size in the transformed space,
the target value for all $d_i$ is 1. The parameter $\alpha$ simplifies to be the average
over all $\delta_i$ and we have
\begin{equation}\label{eq:e}
  e_{\text{STRESS}}
  \coloneqq
  100
  \frac{%
    \sqrt{\sum_{i=1}^n (\alpha - \delta_{i})^2}
  }{
    \sqrt{\sum_{i=1}^n \delta_i^2}
  }.
\end{equation}
In case only the actual ellipses are given (e.g., \cite{luorigg}), one can place a
number of points onto their boundaries (e.g., 8 or 16) and compute the residual with
from them.

\begin{remark}
  While the generation of ellipses from experimental data is useful for visually
  representing color difference data, there are a number of criticisms associated with
  using them in color space evaluation and optimization.
  \begin{itemize}
    \item Ellipses encode the raw experimental data, and perhaps distort it in some way.
      If experimental data is available, using it directly eliminates this layer of
      distortion.
    \item When calculating the STRESS of ellipse data, one has to choose a set of points
      on each ellipse and compute the STRESS for the distances. The final value depends
      on how many values where chosen and where they sit on the ellipses.
    \item Ellipses are two-dimensional geometric objects while color spaces are
      three-dimensional. In many cases, ellipses are drawn orthogonal to the perceived
      lightness dimension, and hence contain no information about lightness, even if the
      original data did.
    \item In some cases, e.g., \cite{macadam1942}, only the chromaticity coordinates $x$
      and $y$ are given for the ellipses, not the lightness component $Y$. The claim is
      that the ellipses don't change signficantly between different levels of lightness
      \cite{brown}. However, this is true at most in a particular range of luminosity
      which is often not provided.
    \end{itemize}
\end{remark}


\subsection{Cost functional for hue linearity}

\begin{figure}
  \centering
  \input{figures/hung-berns-xyy.tex}
  \hfill
  \input{figures/hung-berns-cielab.tex}
  \hfill
  \input{figures/hung-berns-oklab.tex}
  \caption{Hung-Berns \cite{hung} hue linearity data for three color spaces. The points
  which are representable in sRGB are shown in color.}
\end{figure}

There are multiple experiments about which colors are perceived to be of equal
hue~\cite{hung,ebner,xiao} (see figure~\ref{fig:hstress}). Color spaces are considered
good if the transformation maps points of equal perceived hue onto a straight line.

What is a good measure of how well points sit on a straight line?
A common idea is to find the straight line that mimizes the sum of squared distances to
all points. This general approach is referred to as \emph{total least squares (TLS)}.
The line is typically
given implicitly by $\alpha_1 x + \alpha_2 y
= 0$ with $\alpha_1,\alpha_2\in\R$, $\|\alpha\|_2^2 = \alpha_1^2 + \alpha_2^2 = 1$.
Because this assumes that the line passes through the origin, all points are
first translated such that the whitepoint sits in in origin.
For scale-invariance, it is convenient to divide by the maximizer of the squared
distances, in total:
\begin{equation}\label{eq:l}
h_2 \coloneqq
  \frac{
\min_{\|\alpha\|_2=1}
  \sqrt{\sum_{i=1}^n (\alpha_1 \tilde{x}_i + \alpha_2 \tilde{y}_i)^2}
}{
\max_{\|\alpha\|_2=1}
  \sqrt{\sum_{i=1}^n (\alpha_1 \tilde{x}_i + \alpha_2 \tilde{y}_i)^2}
}
\end{equation}
with the translated sample points
\[
  \tilde{x}_i \coloneqq x_i-w_x,\qquad
  \tilde{y}_i \coloneqq y_i-w_y.
\]
The value of $h_2$ can be approximated with any appropriate optimization method. A more
explicit and more easily computable representation however is retrieved as follows.
With the $n$-by-2 coordinate matrix
\[
  A \coloneqq \begin{pmatrix}
    \tilde{x}_1 & \tilde{y}_1\\
    \vdots & \vdots\\
    \tilde{x}_n & \tilde{y}_n
  \end{pmatrix},
\]
the sum in \eqref{eq:l} can be written as
\[
  \sum_{i=1}^n (\alpha_1 \tilde{x}_i + \alpha_2 \tilde{y}_i)^2
  = (A \alpha)^T (A \alpha)
  = \alpha^T A^T A \alpha.
\]
This makes clear that $h_2$ is exactly the ratio of the square roots of the two
eigenvalues of $A^TA$ or equivalently the ratio of the two singular values of $A$,
\[
h_2
= \frac{
  \sqrt{\lambda_{\min}(A^T A)}
  }{
    \sqrt{\lambda_{\max}(A^T A)}
  }
= \frac{\sigma_{\min}(A)}{\sigma_{\max}(A)}.
\]
The value is given explicitly by
\begin{equation*}
  h_2 = \sqrt{
    \frac{
      \xt^T\xt
      + \yt^T\yt
      - \sqrt{(\xt^T\xt - \yt^T\yt)^2 + 4 (\xt^T\yt)^2}
    }{
      \xt^T\xt
      + \yt^T\yt
      + \sqrt{(\xt^T\xt - \yt^T\yt)^2 + 4 (\xt^T\yt)^2}
    }
    }.
\end{equation*}
The expression under the outer root is indeed always nonnegative by virtue of the
Cauchy-Schwarz inequality $(\xt^T\yt)^2 \le (\xt^T\xt) (\yt^T\yt)$. It is 0 if and only
if $\xt$ and $\yt$ are linearly dependent, i.e., if the points $(x_i, y_i)$ sit on a
straight line through the origin.

Since $0\le h_2\le 1$, the value can be given in terms of STRESS,
\begin{equation}\label{eq:hstress}
  h_\text{STRESS} = 100 \frac{\sigma_{\min}(A)}{\sigma_{\max}(A)}.
\end{equation}


% \begin{remark}
%   The representation \eqref{eq:s2} is suitable for optimization purposes. Since a value
%   $\sqrt{t}$ is small if and only if $t$ is small, one would in the interest of
%   simplicity disregard the outer square root.
% \end{remark}


\begin{figure}
  \centering
  \input{figures/hstress.tex}
  \caption{$h_\text{STRESS}$ for different data sets. The dark bar represents the
  average $h_\text{STRESS}$ over all arms in the data set, the light bar indicates the
  maximum.  The largest maxima are CIELAB's and CAM16's famous nonlinearities for blue.
  $J_za_zb_z$ and Oklab perform best for all data sets.}
  \label{fig:hstress}
\end{figure}

\subsection{Cost functional for lightness data}

\cite{fairchildchen}

\begin{figure}
  \centering
  \input{figures/light_stress.tex}
  \caption{lightness stress.}
\end{figure}


\section{Optimization}

It has become fashionable to propose a color space format with free parameters, and then
optimize those parameters against a number of goal functionals. Examples are
\cite{prolab,oklab,jzazbz}.


% \printbibliography{}
\bibliography{main}{}
\bibliographystyle{plain}

\end{document}
